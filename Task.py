"""Task graph framework."""
import types
import collections
import psutil
import traceback
import time
import datetime
import hashlib
import json
import pickle
import os
import logging
import multiprocessing
import threading
import errno
import Queue
import inspect

LOGGER = logging.getLogger('Task')


def _worker(work_queue):
    """Thread worker.  `work_queue` has func/args tuple or 'STOP'."""
    for func, args in iter(work_queue.get, 'STOP'):
        func(*args)


class TaskGraph(object):
    """Encapsulates the worker and tasks states for parallel processing."""

    def __init__(self, token_storage_path, n_workers):
        """Create a task graph.

        Creates an object for building task graphs, executing them,
        parallelizing independent work notes, and avoiding repeated calls.

        Parameters:
            token_storage_path (string): path to a directory where work tokens
                (files) can be stored.  Task graph checks this directory to
                see if a task has already been completed.
            n_workers (int): number of parallel workers to allow during
                task graph execution.  If set to 0, use current process.
        """
        # https://stackoverflow.com/questions/273192/how-can-i-create-a-directory-if-it-does-not-exist
        try:
            os.makedirs(token_storage_path)
        except OSError as exception:
            if exception.errno != errno.EEXIST:
                raise
        self.token_storage_path = token_storage_path
        self.work_queue = Queue.Queue()
        self.n_workers = n_workers
        for thread_id in xrange(n_workers):
            threading.Thread(
                target=TaskGraph.worker, args=(self.work_queue,),
                name=thread_id).start()

        if n_workers > 0:
            self.worker_pool = multiprocessing.Pool(n_workers)
            parent = psutil.Process()
            parent.nice(psutil.BELOW_NORMAL_PRIORITY_CLASS)
            for child in parent.children():
                child.nice(psutil.BELOW_NORMAL_PRIORITY_CLASS)
        else:
            self.worker_pool = None

        # used to lock global resources
        self.global_lock = threading.Lock()

        # if a Task pair is in here, it's been previously created
        self.global_working_task_dict = {}

        self.closed = False

    def __del__(self):
        """Clean up task graph by injecting STOP sentinels."""
        self.close()

    @staticmethod
    def worker(work_queue):
        """Worker taking (func, args, kwargs) tuple from `work_queue`."""
        for func, args, kwargs in iter(work_queue.get, 'STOP'):
            try:
                func(*args, **kwargs)
            except Exception as e:
                LOGGER.error(traceback.format_exc())

    def close(self):
        """Prevent future tasks from being added to the work queue."""
        self.closed = True
        for thread_id in xrange(self.n_workers):
            self.work_queue.put('STOP')

    def add_task(
            self, target=None, args=None, kwargs=None,
            expected_target_path_list=None,
            dependent_task_list=None):
        """Add a task to the task graph.

        Parameters:
            target (function): target function
            args (list): argument list for `target`
            kwargs (dict): keyword arguments for `target`
            expected_target_path_list (list): list of file paths expected to
                be generated by this target and args/kwargs.
            dependent_task_list (list): list of `Task`s that this task is
                dependent on.

        Returns:
            Task which was just added to the graph.
        """
        if self.closed:
            raise ValueError(
                "The task graph is closed and cannot accept more tasks.")
        if args is None:
            args = []
        if kwargs is None:
            kwargs = {}
        if dependent_task_list is None:
            dependent_task_list = []
        if expected_target_path_list is None:
            expected_target_path_list = []
        if target is None:
            target = lambda: None
        task = Task(
            target, args, kwargs, dependent_task_list,
            expected_target_path_list,
            self.token_storage_path)

        with self.global_lock:
            self.global_working_task_dict[task.task_id] = task
        if self.n_workers > 0:
            self.work_queue.put(
                (task,
                 (self.global_lock,
                  self.global_working_task_dict,
                  self.worker_pool),
                 {}))
        else:
            task(
                self.global_lock, self.global_working_task_dict,
                self.worker_pool)
        return task

    def join(self):
        """Join all threads in the graph."""
        for task in self.global_working_task_dict.itervalues():
            task.join()


class Task(object):
    """Encapsulates work/task state for multiprocessing."""

    def __init__(
            self, target, args, kwargs, dependent_task_list,
            expected_target_path_list, token_storage_path):
        """Make a task.

        Parameters:
            target (function): a function that takes the argument list
                `args`
            args (tuple): a list of arguments to pass to `target`.  Can be
                None.
            kwargs (dict): keyword arguments to pass to `target`.  Can be
                None.
            dependent_task_list (list of Task): a list of other
                `Task`s that are to be invoked before `target(args)` is
                invoked.
            expected_target_path_list (list): list of filepaths expected
                to be generated by this target and args/kwargs.
            token_storage_path (string): path to a directory that exists
                where task can store a file to indicate completion of task.
        """
        self.target = target
        self.args = args
        self.kwargs = kwargs
        self.dependent_task_list = dependent_task_list
        self.expected_target_path_list = expected_target_path_list

        # Used to ensure only one attempt at executing and also a mechanism
        # to see when Task is complete
        self.lock = threading.Lock()
        self.lock.acquire()  # the only release is at the end of __call__

        # Make a unique hash of the call
        try:
            if not hasattr(Task, 'target_source_map'):
                Task.target_source_map = {}
            # memoize target source code because it's likely we'll import
            # the same target many times and reflection is slow
            if target not in Task.target_source_map:
                Task.target_source_map[target] = inspect.getsource(target)
            source_code = Task.target_source_map[target]
        except IOError:
            # we might be in a frozen binary, so just leave blank
            source_code = ''
        task_string = '%s:%s:%s:%s:%s:%s' % (
            target.__name__, pickle.dumps(args),
            json.dumps(kwargs, sort_keys=True), source_code,
            expected_target_path_list, _get_file_stats(
                [args, kwargs], expected_target_path_list))
        self.task_id = '%s_%s' % (
            target.__name__, hashlib.sha1(task_string).hexdigest())

        # https://stackoverflow.com/questions/273192/how-can-i-create-a-directory-if-it-does-not-exist
        try:
            os.makedirs(token_storage_path)
        except OSError as exception:
            if exception.errno != errno.EEXIST:
                raise

        # The following file will be written when work is complete
        self.token_path = os.path.join(token_storage_path, self.task_id)

    def __call__(
            self, global_lock, global_working_task_dict,
            global_worker_pool):
        """Invoke this method when ready to execute task.

        Parameters:
            global_lock (threading.Lock): use this to lock global
                the global resources to the task graph.
            global_working_task_dict (dict): contains a dictionary of task_ids
                to Tasks that are currently executing.  Global resource and
                should acquire lock before modifying it.
            global_worker_pool (multiprocessing.Pool): a process pool used to
                execute subprocesses.  If None, use current process.

        Returns:
            None
        """
        try:
            LOGGER.debug("Starting task %s", self.task_id)
            if self.is_complete():
                LOGGER.debug(
                    "Completion token exists for %s so not executing",
                    self.task_id)
                return

            # if this Task is currently running somewhere, wait for it.
            if len(self.dependent_task_list) > 0:
                LOGGER.debug("joining dependent threads %s", self.task_id)
                for task in self.dependent_task_list:
                    task.join()
                    if not task.is_complete():
                        raise RuntimeError(
                            "Task %s didn't complete, discontinuing "
                            "execution of %s" % (
                                task.task_id, self.task_id))

            # Do this task's work

            LOGGER.debug("Starting process for %s", self.task_id)
            if global_worker_pool is not None:
                result = global_worker_pool.apply_async(
                    func=self.target, args=self.args, kwds=self.kwargs)
                result.get()
            else:
                self.target(*self.args, **self.kwargs)
            LOGGER.debug("Complete process for %s", self.task_id)
            with open(self.token_path, 'w') as token_file:
                token_file.write(str(datetime.datetime.now()))
        finally:
            self.lock.release()

    def is_complete(self):
        """Return true if complete token and expected files exist."""
        LOGGER.debug('is_complete %s %s', self.token_path, self.expected_target_path_list)
        return all([
            os.path.exists(path)
            for path in [self.token_path] + self.expected_target_path_list])

    def join(self):
        """Block until task is complete."""
        with self.lock:
            pass


def _get_file_stats(base_value, ignore_list):
    """Iterate over any values that are filepaths by getting filestats."""
    if isinstance(base_value, types.StringType):
        try:
            if base_value not in ignore_list:
                yield (os.path.getmtime(base_value), os.path.getsize(base_value))
        except OSError:
            pass
    elif isinstance(base_value, collections.Mapping):
        for key in sorted(base_value.iterkeys()):
            value = base_value[key]
            for x in _get_file_stats(value, ignore_list):
                if x not in ignore_list:
                    yield x
    elif isinstance(base_value, collections.Iterable):
        for value in base_value:
            for x in _get_file_stats(value, ignore_list):
                if x not in ignore_list:
                    yield x
